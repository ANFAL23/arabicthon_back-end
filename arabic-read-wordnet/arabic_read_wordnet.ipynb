{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0wqcBaGd5T"
      },
      "source": [
        "# Read AraWordNet\n",
        "\n",
        "contributed by **Ali Ahmed**\n",
        "\n",
        "A utility file to read **AraWordNet** and provide dictionary to map between the sense and its words.\n",
        "\n",
        "AraWordNet[1][2] could be found at http://globalwordnet.org/resources/arabic-wordnet/.\n",
        "\n",
        "## Prerequisite:\n",
        "- Define `wordnet_path` variable\n",
        "\n",
        "[1] Black W., Elkateb S., Rodriguez H., Alkhalifa M., Vossen P., Pease A., Bertran M., Fellbaum C., (2006) The Arabic WordNet Project, Proceedings of LREC 2006\n",
        "\n",
        "[2] Lahsen Abouenour, Karim Bouzoubaa, Paolo Rosso (2013) On the evaluation and improvement of Arabic WordNet coverage and usability, Language Resources and Evaluation 47(3) pp 891–91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwMbc8UVGd5X"
      },
      "source": [
        "## Import and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIRD13NwGd5Y",
        "outputId": "1b2fb5f5-8814-44a5-e773-f93e0dba6e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'wordnet_path' (str)\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "wordnet_path = '/content/arb2-lmf.xml'\n",
        "%store wordnet_path\n",
        "%store -r wordnet_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OK005bRGd5Z"
      },
      "source": [
        "## Read AraWordNet\n",
        "\n",
        "**AraWordNet** has the following structure which is embedded in XML:\n",
        "- LexicalEntry:\n",
        "  - Lemma. Its properties are: `partOfSpeech` and `writtenForm`. We are interested in the `writtenForm` which shows how the word looks like.\n",
        "  - Sense. Its properties are: `id` and `synset`. We are interested in the `synset` to map between the word and its relations in the WordNet.\n",
        "  - WordForm. Its properties are: `formType` and `writtenForm`. We are not interested in any of these properties.\n",
        "- Synset. Its properties are `baseConcept` and `id`. We are interested in the `id` which maps to the `synset` property in the `Sense` node for every word:\n",
        "  - SynsetRelations\n",
        "    - SynsetRelation. Its properties are `relType` and `targets`. We are interested in both. `relType` shows the relation type (whether its `hypernym`, `hyponym`, .. etc). `targets` maps to the `synset` property in the `Sense` node for every word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoFmkUIoGd5a",
        "outputId": "2f6d4bd3-cac3-4bd8-f15f-692f262c5fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading AraWordNet\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading AraWordNet\")\n",
        "wordnet_file = open(wordnet_path).read()\n",
        "wordnet = BeautifulSoup(wordnet_file, \"xml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HL0-6gsGd5b"
      },
      "source": [
        "## Extracting relations from AraWordNet (AWN)\n",
        "\n",
        "Relation types can be:\n",
        "- `hypernym`: represents a parent to child relationship.\n",
        "- `hyponym`: represents a child to parent relationship.\n",
        "- `has_instance`: represents an object to one of its instances relationship.\n",
        "- `is_instance`: represents an instance to its object relationship.\n",
        "\n",
        "and other relationships that we are not interested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXXmZSZuGd5b",
        "outputId": "47fac361-fd7a-41eb-ad08-5aeabbdc87d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading hypernym relations\n"
          ]
        }
      ],
      "source": [
        "print(\"Reading hypernym relations\")\n",
        "relations = []\n",
        "# For every synonym set\n",
        "for synset in wordnet.findAll('Synset'):\n",
        "    # Get its hypernym relations\n",
        "    synset_hypernym_relations = list(filter((lambda relation: relation['relType'] == 'hypernym'), synset.findAll('SynsetRelation')))\n",
        "    for relation in synset_hypernym_relations:\n",
        "        # Construct a pair between each synonym set and its child\n",
        "        relations.append((synset['id'], relation['targets']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9_6pDY5Gd5c"
      },
      "source": [
        "### Testing if hypernym relations represent similar relations as hyponym relations\n",
        "\n",
        "If this is true, we can safely ignore the `hyponym` relationship and work with the `hypernym` relationship only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s13LpmLLGd5c",
        "outputId": "0aedee43-23d3-4857-b6f4-ebafd4a89abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: Are hypernym relations similar to hyponym relations? True\n",
            "Considering hypernym relations only\n"
          ]
        }
      ],
      "source": [
        "hyponym_relations = []\n",
        "# For every synonym set\n",
        "for synset in wordnet.findAll('Synset'):\n",
        "    # Get its hyponym relations\n",
        "    synset_hyponym_relations = list(filter((lambda relation: relation['relType'] == 'hyponym'), synset.findAll('SynsetRelation')))\n",
        "    for relation in synset_hyponym_relations:\n",
        "        # Construct a pair between each synonym set and its parent\n",
        "        hyponym_relations.append((relation['targets'], synset['id']))\n",
        "\n",
        "relations.sort()\n",
        "hyponym_relations.sort()\n",
        "print(\"Test: Are hypernym relations similar to hyponym relations? {}\".format(relations == hyponym_relations))\n",
        "if relations == hyponym_relations: print(\"Considering hypernym relations only\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGh7aD8QGd5d"
      },
      "source": [
        "### We might also consider the is_instance and has_instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD_bQBf2Gd5d"
      },
      "outputs": [],
      "source": [
        "has_instance_relations = []\n",
        "# For every synonym set\n",
        "for synset in wordnet.findAll('Synset'):\n",
        "    # Get its has_instance relations\n",
        "    synset_has_instance_relations = list(filter((lambda relation: relation['relType'] == 'has_instance'), synset.findAll('SynsetRelation')))\n",
        "    for relation in synset_has_instance_relations:\n",
        "        # Construct a pair between each synonym set and its instance\n",
        "        has_instance_relations.append((synset['id'], relation['targets']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPyji6CzGd5d"
      },
      "source": [
        "### Testing if has_instance relations represent similar relations as is_instance relations\n",
        "\n",
        "If this is true, we can again safely ignore the `is_instance` relationship and work with the `has_instance` relationship only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcRqQ4jGGd5e",
        "outputId": "6dcad8a9-55ce-4b4c-d13e-b40a7cc5bda7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_instance_relations = []\n",
        "# For every synonym set\n",
        "for synset in wordnet.findAll('Synset'):\n",
        "    # Get its is_instance relations\n",
        "    synset_is_instance_relations = list(filter((lambda relation: relation['relType'] == 'is_instance'), synset.findAll('SynsetRelation')))\n",
        "    for relation in synset_is_instance_relations:\n",
        "        # Construct a pair between each synonym set and its object\n",
        "        is_instance_relations.append((relation['targets'], synset['id']))\n",
        "\n",
        "is_instance_relations.sort()\n",
        "has_instance_relations.sort()\n",
        "is_instance_relations == has_instance_relations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdsGTBAwGd5e"
      },
      "source": [
        "### Testing if hypernym contains repeated relations\n",
        "\n",
        "We have to remove repeated relations if they exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L45H0jeUGd5e",
        "outputId": "3d479630-1ce1-4255-9e2d-c9f277227547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of hypernym relations: 19806\n",
            "Contains unique relations only? False\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of hypernym relations: {}\".format(len(relations)))\n",
        "print(\"Contains unique relations only? {}\".format(len(relations) == len(set(hyponym_relations))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhqXJiZmGd5f"
      },
      "source": [
        "### Therefore, We have to consider the set of unique relations ignoring the repeated ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0BK9-qpGd5f",
        "outputId": "42232060-0302-4882-b367-e6e4abefab89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Considering unique hypernym relations only\n",
            "Number of unique hypernym relations: 9305\n"
          ]
        }
      ],
      "source": [
        "print(\"Considering unique hypernym relations only\")\n",
        "relations = list(set(relations))\n",
        "print(\"Number of unique hypernym relations: {}\".format(len(relations)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l48l_ocaGd5f"
      },
      "source": [
        "### Testing if we have self- or bi-directional relations, and removing them\n",
        "Self-directional is a relation between the word and itself. Bi-directional is a relation between two words where every one of them is parent `hypernym` of the other. Both relations contain loops and will be problematic when constructing tree for generating catcode and word-sense-children files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN5Z-MVtGd5f",
        "outputId": "c60defce-3231-4a91-f5ea-27f74aa02eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Considering unique uni-directional hypernym relations only\n",
            "Number of unique uni-directional hypernym relations: 9302\n"
          ]
        }
      ],
      "source": [
        "# List for the relations in both directions\n",
        "bi_directional_relations = []\n",
        "# Synset is the sense id for the parent, target is the sense id for the child\n",
        "for synset, target in relations:\n",
        "    # Add a relation between the parent and the child\n",
        "    bi_directional_relations.append((synset, target))\n",
        "    # Add a relation in the other way around\n",
        "    bi_directional_relations.append((target, synset))\n",
        "\n",
        "# Count the number of occurences for each pair. This should be 1 for every pair since we are\n",
        "# considering the unique set of hypernym relations\n",
        "counter = Counter(bi_directional_relations)\n",
        "# If the counter of any pair is more than 1, it should be marked as invalid\n",
        "invalid_relations = list(filter((lambda relation: counter[relation] > 1), bi_directional_relations))\n",
        "print(\"Considering unique uni-directional hypernym relations only\")\n",
        "# Remove the invalid relations\n",
        "relations = list(set(relations) - set(invalid_relations))\n",
        "print(\"Number of unique uni-directional hypernym relations: {}\".format(len(relations)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owV_CfJlGd5g"
      },
      "source": [
        "### Testing if every child is occuring once as a child\n",
        "\n",
        "Child should have only one parent and therefore should occur in the unique uni-directional relations once. If child occur multiple times as child, this means the parents n-balls will have to intersect. As a result, we have to remove the relations containing repeated children."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvRUJ4OsGd5g",
        "outputId": "8ee67596-63dd-4f4a-cbb7-41862ae66482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of invalid children: 1763\n"
          ]
        }
      ],
      "source": [
        "children = list(map((lambda relation: relation[1]), relations))\n",
        "counter = Counter(children)\n",
        "invalid_children = list(filter((lambda child: counter[child] > 1), children))\n",
        "\n",
        "print(\"Number of invalid children: {}\".format(len(invalid_children)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixWy9jj7Gd5g",
        "outputId": "f87e36b7-43e1-4345-dfa0-75ec31bd4e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of valid hypernym relations without repeated children: 7177\n"
          ]
        }
      ],
      "source": [
        "relations = list(filter((lambda relation: relation[0] not in invalid_children), relations))\n",
        "relations = list(filter((lambda relation: relation[1] not in invalid_children), relations))\n",
        "print(\"Number of valid hypernym relations without repeated children: {}\".format(len(relations)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsrJL8jjGd5h"
      },
      "source": [
        "### Extract vocabulary\n",
        "\n",
        "Our vocabulary is limited to those words appearing in the valid hypernym relations. We have to extract them as they are providing the written form which is used in the word embedding file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqFTX1SFGd5h",
        "outputId": "04c946bd-c529-4072-8cbf-5a244943f655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of synonym set ids: 7622\n"
          ]
        }
      ],
      "source": [
        "# List for synonym set ids\n",
        "synset_ids = []\n",
        "for relation in relations:\n",
        "    # Extract parent synonym set id\n",
        "    synset_ids.append(relation[0])\n",
        "    # Extract child synonym set id\n",
        "    synset_ids.append(relation[1])\n",
        "\n",
        "# Create unique set of synonym set ids extracted from the valid hypernym relations\n",
        "synset_ids = list(set(synset_ids))\n",
        "print(\"Number of synonym set ids: {}\".format(len(synset_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJSa6L3OGd5h",
        "outputId": "5d411c01-2cd6-4633-b604-c639974fafba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique words: 14391\n"
          ]
        }
      ],
      "source": [
        "# Filter lexical entries which have synonym set id appearing in our list\n",
        "lexical_entries = list(filter((lambda entry: entry.Sense['synset'] in synset_ids), wordnet.findAll('LexicalEntry')))\n",
        "# Extract words that correspond to our synonym set id list. These words form our vocabulary list.\n",
        "words = list(set(map((lambda entry: entry.Lemma['writtenForm']), lexical_entries)))\n",
        "print(\"Number of unique words: {}\".format(len(words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN_WMZSxGd5i"
      },
      "source": [
        "## Dictinary for synset to words\n",
        "\n",
        "Construct a dictionary for every synonym set id and its set of words. The key is the synonym set id and the value is a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxfbxcD3Gd5i"
      },
      "outputs": [],
      "source": [
        "lexical_entries = list(filter((lambda entry: entry.Sense['synset'] in synset_ids), wordnet.findAll('LexicalEntry')))\n",
        "synset_dict = defaultdict(list)\n",
        "for entry in lexical_entries:\n",
        "    written_form = entry.Lemma['writtenForm']\n",
        "    synset_dict[entry.Sense['synset']].append(written_form)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEr-BHDmIkBc"
      },
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-bTNwtfIm0U",
        "outputId": "66cd0aac-afad-412b-ba0f-8e7b5b6099a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing synset to words dictionary:\n",
            "Synset: ZalAam_n1AR\n",
            "Words: [' ظلْماء', ' دُهْمة', 'عتْمة', 'ظلام', 'ظُلْمة', 'غلس', 'قتْمة', ' ظَلْماء', ' دُهْمَة']\n",
            "\n",
            "Testing vocabulary list:\n",
            "Total words in the vocabulary: 14391\n",
            "Sample words: ['', 'إِتْقان', 'أسْلُوب كِتابِي', 'قبْر', 'سير في موكب', 'وِقَاء', 'سخّن', 'شوى', 'عالج ببراعة', 'قُماش قُطْنِي']\n",
            "\n",
            "Testing unique uni-directional hypernym relations:\n",
            "Number of unique uni-directional hypernym relations: 7177\n",
            "Sample relations: [('Eamaliy~ap_n1AR', 'jamoE__n1AR'), ('>abodaEa_*ihoniy~aA_v1AR', 'taxay~ala_v2AR'), ('>amad_n1AR', 'madaY_n1AR'), ('quw~ap_n7AR', 'taHak~um_n2AR'), ('tijaArap_n4AR', 'tijaArap_n6AR'), ('Eil~ap_n2AR', 'xalal__n1AR'), ('quw~ap_n3AR', 'jA*iby~ap _n1AR'), ('baAsotA_n1AR', 'lAzAnyA_n1AR'), ('mud~ap_muHad~adFp_n1AR', 'faSol__n1AR'), ('Tariyqap_n2AR', 'Hal~_n1AR')]\n"
          ]
        }
      ],
      "source": [
        "# Test the synset to words dictionary\n",
        "print(\"Testing synset to words dictionary:\")\n",
        "test_synset = list(synset_dict.keys())[0]\n",
        "print(f\"Synset: {test_synset}\")\n",
        "print(f\"Words: {synset_dict[test_synset]}\")\n",
        "\n",
        "# Test the vocabulary list\n",
        "print(\"\\nTesting vocabulary list:\")\n",
        "print(f\"Total words in the vocabulary: {len(words)}\")\n",
        "print(f\"Sample words: {words[:10]}\")\n",
        "\n",
        "# Test the unique uni-directional hypernym relations\n",
        "print(\"\\nTesting unique uni-directional hypernym relations:\")\n",
        "print(f\"Number of unique uni-directional hypernym relations: {len(relations)}\")\n",
        "print(f\"Sample relations: {relations[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6futgaBNLOj",
        "outputId": "172352fb-467f-4e14-af2c-3452c2cc6187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing synset to words dictionary:\n",
            "Synset: $a>n_n1AR\n",
            "Words: ['شأن', 'همّ']\n",
            "\n",
            "Testing vocabulary list:\n",
            "Total words in the vocabulary: 14391\n",
            "Sample words: ['', 'إِتْقان', 'أسْلُوب كِتابِي', 'قبْر', 'سير في موكب', 'وِقَاء', 'سخّن', 'شوى', 'عالج ببراعة', 'قُماش قُطْنِي']\n",
            "\n",
            "Testing unique uni-directional hypernym relations:\n",
            "Number of unique uni-directional hypernym relations: 7177\n",
            "Sample relations: [('Eamaliy~ap_n1AR', 'jamoE__n1AR'), ('>abodaEa_*ihoniy~aA_v1AR', 'taxay~ala_v2AR'), ('>amad_n1AR', 'madaY_n1AR'), ('quw~ap_n7AR', 'taHak~um_n2AR'), ('tijaArap_n4AR', 'tijaArap_n6AR'), ('Eil~ap_n2AR', 'xalal__n1AR'), ('quw~ap_n3AR', 'jA*iby~ap _n1AR'), ('baAsotA_n1AR', 'lAzAnyA_n1AR'), ('mud~ap_muHad~adFp_n1AR', 'faSol__n1AR'), ('Tariyqap_n2AR', 'Hal~_n1AR')]\n"
          ]
        }
      ],
      "source": [
        "# Test the synset to words dictionary\n",
        "print(\"Testing synset to words dictionary:\")\n",
        "test_synset = list(synset_dict.keys())[5]\n",
        "print(f\"Synset: {test_synset}\")\n",
        "print(f\"Words: {synset_dict[test_synset]}\")\n",
        "\n",
        "# Test the vocabulary list\n",
        "print(\"\\nTesting vocabulary list:\")\n",
        "print(f\"Total words in the vocabulary: {len(words)}\")\n",
        "print(f\"Sample words: {words[:10]}\")\n",
        "\n",
        "# Test the unique uni-directional hypernym relations\n",
        "print(\"\\nTesting unique uni-directional hypernym relations:\")\n",
        "print(f\"Number of unique uni-directional hypernym relations: {len(relations)}\")\n",
        "print(f\"Sample relations: {relations[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzRipQdWLkbl",
        "outputId": "1ede594f-c0a0-415f-e58a-2574d11ba09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words related to 'قبْر' based on hyponym:\n",
            "['بُقْعة', 'مكان', 'نُقْطة طُوبُوغْرافِيّة']\n"
          ]
        }
      ],
      "source": [
        "def get_related_words(word, relation_type):\n",
        "    \"\"\"\n",
        "    Get words related to the given word based on the chosen relation type.\n",
        "\n",
        "    Parameters:\n",
        "    - word: The input word.\n",
        "    - relation_type: The type of relation (hypernym, hyponym, has_instance, is_instance).\n",
        "\n",
        "    Returns:\n",
        "    - A list of words related to the input word based on the chosen relation type.\n",
        "    \"\"\"\n",
        "    related_words = []\n",
        "\n",
        "    # Find the synset ID for the input word\n",
        "    synset_id = None\n",
        "    for synset, words in synset_dict.items():\n",
        "        if word in words:\n",
        "            synset_id = synset\n",
        "            break\n",
        "\n",
        "    if synset_id is not None:\n",
        "        # Find related synsets based on the chosen relation type\n",
        "        if relation_type == 'hypernym':\n",
        "            related_synsets = [target for source, target in relations if source == synset_id]\n",
        "        elif relation_type == 'hyponym':\n",
        "            related_synsets = [source for source, target in relations if target == synset_id]\n",
        "        elif relation_type == 'has_instance':\n",
        "            related_synsets = [target for source, target in has_instance_relations if source == synset_id]\n",
        "        elif relation_type == 'is_instance':\n",
        "            related_synsets = [source for source, target in is_instance_relations if target == synset_id]\n",
        "        else:\n",
        "            print(\"Invalid relation type. Choose from hypernym, hyponym, has_instance, or is_instance.\")\n",
        "            return []\n",
        "\n",
        "        # Get words corresponding to the related synsets\n",
        "        related_words = [word for synset, words in synset_dict.items() if synset in related_synsets for word in words]\n",
        "\n",
        "    return related_words\n",
        "\n",
        "# Example usage:\n",
        "input_word = \"قبْر\"\n",
        "chosen_relation_type = \"hyponym\"  # Choose from hypernym, hyponym, has_instance, or is_instance\n",
        "result_words = get_related_words(input_word, chosen_relation_type)\n",
        "\n",
        "print(f\"Words related to '{input_word}' based on {chosen_relation_type}:\")\n",
        "print(result_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIhcHsO6NAhN",
        "outputId": "a5c21a3f-7089-4ce2-d408-add21082ef52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words related to 'مكان' based on hyponym:\n",
            "['حِلّة', 'بيْت', 'منْزِل', 'مقام', 'مقرّ', 'مرْكز', 'مسْكن', 'مسْكن', 'سكن', 'وَطَن']\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "input_word = \"مكان\"\n",
        "chosen_relation_type = \"hyponym\"  # Choose from hypernym, hyponym, has_instance, or is_instance\n",
        "result_words = get_related_words(input_word, chosen_relation_type)\n",
        "\n",
        "print(f\"Words related to '{input_word}' based on {chosen_relation_type}:\")\n",
        "print(result_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HguRQaq7PdV4"
      },
      "source": [
        "# **Load Arabic Word-Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc0qtT7fPfSz"
      },
      "source": [
        "A utility to load word embedding model.\n",
        "\n",
        "AraVec N-Gram model is used as a source of word embeddings, as it provides larger set of embeddings to the words we have in AraWordNet. fastText and AraVec uni-gram are both uni-gram models so they are missing many words in the WordNet. In this notebook, we load the the N-gram model.\n",
        "\n",
        "Prerequisite:\n",
        "Define fasttext_path, uni_gram_aravec_path and n_gram_aravec_path variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EAxn8ooPkYC"
      },
      "source": [
        "## Import and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsi0_jndQip5",
        "outputId": "d1abeee4-00c0-4294-cccf-2184b6324ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=7a560ee0c7d04e0850ce9408e873b6e1329193888d3c94fb9aa3668f4bc5fd5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFb8n2HjQV_i",
        "outputId": "4682c200-0647-412f-bbb8-ab3996125218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('ar', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.ar.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SkY9xg-zhMv5",
        "outputId": "061e773c-c1b3-4f12-cef8-c54483536111"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('ar', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.ar.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a85rHZ02Pmax",
        "outputId": "d00a77a4-775c-4678-9d3b-3f0cd5885a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'fasttext_path' (str)\n",
            "Stored 'uni_gram_aravec_path' (str)\n",
            "Stored 'n_gram_aravec_path' (str)\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import gensim\n",
        "\n",
        "fasttext_path = '/content/cc.ar.300.bin'\n",
        "uni_gram_aravec_path = '/content/full_grams_sg_wiki.mdl'\n",
        "n_gram_aravec_path = uni_gram_aravec_path\n",
        "\n",
        "%store fasttext_path\n",
        "%store -r fasttext_path\n",
        "%store uni_gram_aravec_path\n",
        "%store -r uni_gram_aravec_path\n",
        "%store n_gram_aravec_path\n",
        "%store -r n_gram_aravec_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aamm8XoxP06e"
      },
      "source": [
        "Load fastText word embedding model\n",
        "## Nouvelle section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6f9RTuaP48k"
      },
      "source": [
        "fastText[1] word embedding could be found at https://fasttext.cc/docs/en/crawl-vectors.html. We use the Arabic word embedding.\n",
        "\n",
        "[1] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, “Learning Word Vectors for 157 Languages”, in Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7hCC919g4cE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import fasttext\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(word, related_words, model):\n",
        "    similarities = []\n",
        "    for related_word in related_words:\n",
        "        similarity = np.dot(model[word], model[related_word]) / (np.linalg.norm(model[word]) * np.linalg.norm(model[related_word]))\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # Create a DataFrame with words and cosine similarities\n",
        "    result_df = pd.DataFrame({'Words': related_words, 'Cosine_Similarity': similarities})\n",
        "    return result_df\n",
        "\n",
        "# Example usage\n",
        "input_word = 'مكان'  # Replace with the desired word\n",
        "related_words = ['حِلّة', 'بيْت', 'منْزِل', 'مقام', 'مقرّ', 'مرْكز', 'مسْكن', 'مسْكن', 'سكن', 'وَطَن']  # Replace with your list of related words\n",
        "\n",
        "# Calculate cosine similarity\n",
        "result_df = cosine_similarity(input_word, related_words, ft)\n",
        "\n",
        "# Print the result DataFrame\n",
        "print(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6woukCMP5tf"
      },
      "outputs": [],
      "source": [
        "# Code to load the model, the code is imported from fasttext: https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
        "    return data\n",
        "\n",
        "fasttext_model = load_vectors(fasttext_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdC1j_YMP7n6"
      },
      "source": [
        "## Load AraVec Uni-Gram word embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SQuKDOHQAbk"
      },
      "source": [
        "AraVec[2] uni-gram word embedding could be found at https://github.com/bakrianoo/aravec#unigrams-models. We use the Wikipedia-SkipGram with vector size 300.\n",
        "\n",
        "[2] A. Soliman, K. Eisa, and S. R. El-Beltagy, “AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP”, in proceedings of the 3rd International Conference on Arabic Computational Linguistics (ACLing 2017), Dubai, UAE, 2017."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhJ4PSy3QBxP"
      },
      "outputs": [],
      "source": [
        "unigram_aravec_model = gensim.models.Word2Vec.load(uni_gram_aravec_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgKaHcpGQEko"
      },
      "source": [
        "## Load AraVec N-Gram word embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onpRG1RqQIDn"
      },
      "source": [
        "AraVec n-gram word embedding could be found at https://github.com/bakrianoo/aravec#n-grams-models-1. We use the Wikipedia-SkipGram with vector size 300."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-kB1yYlQFrf"
      },
      "outputs": [],
      "source": [
        "ngram_aravec_model = gensim.models.Word2Vec.load(n_gram_aravec_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFiZb69SRaxy"
      },
      "source": [
        "# **Cosine similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K53E3auJRdzQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "related_words = result_words\n",
        "\n",
        "def calculate_cosine_similarity(input_word, related_words, model):\n",
        "    vector_input_word = model[input_word]\n",
        "    vectors_related_words = [model[word] for word in related_words]\n",
        "\n",
        "    cosine_similarities = cosine_similarity([vector_input_word], vectors_related_words)[0]\n",
        "\n",
        "    result_df = pd.DataFrame({'Words': related_words, 'Cosine Similarity': cosine_similarities})\n",
        "    return result_df\n",
        "\n",
        "# Calculate cosine similarity using the fastText model\n",
        "df = calculate_cosine_similarity(input_word, related_words, ft)\n",
        "\n",
        "# Print the result DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfkeWPZWN6yH"
      },
      "source": [
        "# **WordCloud for مكان hyponym**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPgWL1gsSKS5"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3KTSigFST6R"
      },
      "source": [
        "## Import arabic font"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o4_FoKMSOqM"
      },
      "outputs": [],
      "source": [
        "arabic_font_path = \"NotoKufiArabic-VariableFont_wght.ttf\"\n",
        "arabic_font = {\"font_path\": arabic_font_path, \"width\": 800, \"height\": 400, \"background_color\": 'white'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfMaSxkmSYY5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Create a WordCloud\n",
        "wordcloud = WordCloud(**arabic_font).generate_from_frequencies(\n",
        "        dict(zip(df['words'], df['cosine']))\n",
        "    )\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KliPUJMQ10J"
      },
      "source": [
        "## **References**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3GyzpT3Q2pX"
      },
      "source": [
        "https://github.com/bakrianoo/aravec#unigrams-models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYmuOjKXGd5i"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
        "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
        "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
        "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
        "        </a>\n",
        "        &copy; T. Dong, C. Bauckhage<br/>\n",
        "        Licensed under a\n",
        "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
        "            CC BY-NC 4.0\n",
        "        </a>.\n",
        "      </td>\n",
        "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
        "          <b>Acknowledgments:</b>\n",
        "          This material was prepared within the project\n",
        "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
        "              P3ML\n",
        "          </a>\n",
        "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
        "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
        "      </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}